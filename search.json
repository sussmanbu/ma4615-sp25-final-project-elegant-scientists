[
  {
    "objectID": "posts/2025-04-30-blog-8/blog-8.html",
    "href": "posts/2025-04-30-blog-8/blog-8.html",
    "title": "Blog 8: A pivot at the end",
    "section": "",
    "text": "Continuing Exploratory Data Analtsis… Sort of…\nWe ran into an issue late in the semester. Our iriginal dataset included only three air quality monitoring stations around Boston. These sites and associated trends have been well described in earlier blog posts. However, we wanted to draw broader conclusions about associations between demographic characteristics and exposure to air pollution than was possible with that geographic spread."
  },
  {
    "objectID": "posts/2025-04-14-blog-6/blog-6.html",
    "href": "posts/2025-04-14-blog-6/blog-6.html",
    "title": "Blog 6: Developing a GLM",
    "section": "",
    "text": "Choosing model type\nInitially, we wanted to pursue a Gamma model for predicting PM2.5 based on racial distribution in the different Boston neighborhoods. However, the structure of our data created an issue with this model type.\nAs shown below, the distribution of PM2.5 is skewed right, consistent with many other air pollution distributions. This suggests that most days have relatively low [PM2.5], with infrequent events of high particulate matter.\n\nThere was also a general trend of increasing variance with increased mean.\n Both the distribution and the variance-to-mean relationship fit a gamma model. The problem comes from the existence of true zero values within the PM2.5 daily mean measurements. Additionally, there were three instances of negative [PM2.5] values in the data, which we set equal to zero under the assumption that negative particulate matter cannot exist.\nGamma models cannot handle true zero values. The alternative we found was to use a Tweedie model, another form of GLM that allows for true zeroes.\n\n\nCreating the Tweedie models\nWe wanted to explore how racial distribution was related to particulate matter exposure. When developing our models, it was important to consider multicollinearity within our data. For example, we could not include total population in addition to a count of each race, as those values add up to the total population of each neighborhood. A similar issue arises with percent by race, as all the racial percentages add up to 100%.\nTo solve this issue, we tried two approaches, controlling for variation by month.\n\n1. Relative percent\nThis model used percent by race as predictor variables while purposely excluding Percent White in order for that group to serve as a reference.\nWe found that PM2.5 increased throughout the year, likely related to increased activity later in 2020 after the most significant lockdowns.\nWe also found that Percent Black and Percent Asian were the only significant groups relative to Percent White.\nControlling for month, compared to neighborhoods with higher percentages of white residents, those with higher proportions of Black or Asian residents tended to experience lower daily [PM2.5].\nThe baseling log-PM2.5 (intercept) was 2.22 µg/m3\nFor every 1% increase in the Black population relative to the white population, PM2.5 decreases by ~1.1%.\nSimilarly, for every 1% increase in the Asian population relative to the white population, PM2.5 decreases by ~1.9%.\n\n\n2. Simpson Diversity Index\nWe also created a Tweedie model using the Simpson Diversity Index, a measure of the variability of type of an individual (in this case by race) within a community.\nWe again found seasonal trends in the [PM2.5]. Controlling for month, we found that the trend in Simpson’s diversity was negative (-0.5966, p &lt; 0.005), indicating that as racial diversity increases, exposure to PM2.5 decreases.\n\n\n\nFindings\nOur initial hypothesis was that more diverse neighborhoods in Boston would be at increased risk of PM2.5 exposure. This hypothesis was based on relative income of these neighborhoods and the potential for increased industrial activity in those areas. However, we have found that the neighborhoods with a higher percent white population had greater exposure to PM2.5 in 2020."
  },
  {
    "objectID": "posts/2025-03-31-blog-4/blog-4.html",
    "href": "posts/2025-03-31-blog-4/blog-4.html",
    "title": "Blog 4: Continuing Data Analysis",
    "section": "",
    "text": "This week, our focus has been on two main goals:\n\nAccess and download census data for racial and socioeconomic variables in neighborhoods surrounding the EPA air quality monitors.\nIdentify seasonal and spatial variability in PM2.5 concentrations based on daily means.\n\n\nCensus Data\nAccording to the Massachusetts DEP 2017 Air Monitoring Network Plan, most PM2.5 sensors operate at a “neighborhood” scale, with a radius of around 0.5-4 km. Using this information, we identified census tracts that intersected a 0.5 km radius around each sensor location. We chose a conservative radius here, knowing that these intersecting tracts extended outside of that 0.5 km radius. This kept the boundaries of included tracts within the “neighborhood” scale radius.\nUsing the ’“tidycensus” library, we identified key variables including total population and race counts, median income, and poverty status. Then we used an API request to create a dataset of desired variables within the tracts that we had identified.\nAs a preliminary investigation, we graphed the demographics of each site by racial percentage of population.\n\nWe then combined that census dataset with our existing air quality data. Our next steps here will be to investigate the variability in PM2.5 concentrations based on racial percentages and socioeconomic status.\n\n\nAir Quality Drivers\nBefore starting on demographic explorations of pollution exposure, we wanted to determine if there was variability in PM2.5 concentration within the neighborhoods of Boston. We also wanted to explore the possibility of seasonal variability in particulate matter concentration.\n\nWe developed a fixed-effect linear mixed model to identify trends in PM2.5 concentration by site and season. We chose a LMM with interaction because it allows for variation in daily mean PM2.5.\n\n\n\nFixed Effects from Linear Mixed Model\n\n\n\n\n\n\n\n\n\n\n\neffect\nterm\nestimate\nstd.error\nstatistic\nconf.low\nconf.high\n\n\n\n\nfixed\n(Intercept)\n4.908\n0.406\n12.082\n4.112\n5.704\n\n\nfixed\nsite_nameDUDLEY SQUARE ROXBURY\n-0.865\n0.310\n-2.789\n-1.473\n-0.257\n\n\nfixed\nsite_nameVON HILLERN ST\n1.156\n0.305\n3.792\n0.559\n1.754\n\n\nfixed\nseasonSummer\n1.720\n0.510\n3.373\n0.721\n2.720\n\n\nfixed\nseasonFall\n1.230\n0.512\n2.405\n0.228\n2.233\n\n\nfixed\nseasonWinter\n1.106\n0.526\n2.105\n0.076\n2.136\n\n\nfixed\nsite_nameDUDLEY SQUARE ROXBURY:seasonSummer\n0.779\n0.359\n2.168\n0.075\n1.484\n\n\nfixed\nsite_nameVON HILLERN ST:seasonSummer\n0.802\n0.344\n2.329\n0.127\n1.476\n\n\nfixed\nsite_nameDUDLEY SQUARE ROXBURY:seasonFall\n1.309\n0.358\n3.661\n0.608\n2.010\n\n\nfixed\nsite_nameVON HILLERN ST:seasonFall\n0.590\n0.343\n1.720\n-0.083\n1.263\n\n\nfixed\nsite_nameDUDLEY SQUARE ROXBURY:seasonWinter\n1.322\n0.376\n3.517\n0.585\n2.059\n\n\nfixed\nsite_nameVON HILLERN ST:seasonWinter\n0.901\n0.364\n2.476\n0.188\n1.614\n\n\n\n\n\nThe output of this model is shown in the table above. The baseline for this model is Kenmore Sq. in Springtime. Each combination of Site and Season (mean) is compared to that baseline PM2.5 concentration (4.91 µg/m3). Each comparison accounts for significant daily variation in PM2.5 concentration.\n\nMain Take-Aways\n\nRoxbury generally demonstrated lower PM2.5 concentration than Kenmore.\nDorchester generally demonstrated higher PM2.5 concentration than Kenmore.\nSeasonal trends varied by station.\n\nIt is important to note that our data is from 2020, during the height of the COVID-19 pandemic. In particular, we see Spring 2020 (March, April, May) shows the lowest concentrations of PM2.5. This is unsurprising, as the majority of lockdown time occurred during that spring. These trends may not be reproducible across other years of data.\n\n\n\nNext Steps\nThis week, we combined census data with air quality parameters and identified underlying trends in the distribution of particulate matter throughout Boston.\nNext, we will develop a gamma model to predict demographic impact of varying particulate matter concentrations throughout the year.\nOne challenge we anticipate is the temporal disparity between out datasets. PM2.5 is measured hourly, aggregated daily (we downloaded daily means), and then summarized over monthly or seasonal scales. Census data is only measured on an annual scale. It is possible that seasonal variability would be missed if air quality data were to be averaged to a comparable temporal resolution."
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog2_data_clean.html",
    "href": "posts/2025-03-17-blog-2/blog2_data_clean.html",
    "title": "Blog 2: Background, Data Cleaning, Data Equity",
    "section": "",
    "text": "The EPA AirData project monitors air quality across the U.S., Puerto Rico, and the U.S. Virgin Islands. It utilizes the Index Report through an annual summary of Air Quality Index (AQI) values in any county or city. The summary contains maximum, 90th percentile and median AQI, the count of days in each AQI category, and the count of days of when the AQI could be due to a criteria pollutant. They also provide a statistics report that shows the annual summary of air pollution values showing the highest values reported during the year by all monitors in the Core Based Statistical Area (CBSA) or county.\nAir quality stations monitor up to six pollutants, which include ozone, carbon monoxide, nitrogen dioxide, sulfur dioxide, lead, and particulate matter (PM) in sizes 10 and 2.5 micrometers. This study will focus on particulate matter, particularly PM2.5 or fine particulate matter. These small particulates are a mix of solids and aerosols that can have negative impacts on human health and the environment. Most fine particles (PM2.5) are derived from the combustion of fossil fuels or organic materials (i.e. wood), but can also form through chemical reactions in the atmosphere CRB. These pollutants can be inhaled and lead to adverse health effects like reduced lung function, or lung cancer IARC.\nAirData uses AQI for some of its reports, as it is an important index for measuring and reporting daily air quality. Based on a scale from 0 to 500, one can tell how clean or polluted the air is in their area. The higher the AQI value, the more polluted the air is, posing a greater risk to people’s health. An AQI of below 100 is seen as satisfactory, and above 100 is unhealthy, first for sensitive groups of people and then for the general public. Anything greater than 300 is considered hazardous for the entire population."
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-background",
    "href": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-background",
    "title": "Blog 2: Background, Data Cleaning, Data Equity",
    "section": "",
    "text": "The EPA AirData project monitors air quality across the U.S., Puerto Rico, and the U.S. Virgin Islands. It utilizes the Index Report through an annual summary of Air Quality Index (AQI) values in any county or city. The summary contains maximum, 90th percentile and median AQI, the count of days in each AQI category, and the count of days of when the AQI could be due to a criteria pollutant. They also provide a statistics report that shows the annual summary of air pollution values showing the highest values reported during the year by all monitors in the Core Based Statistical Area (CBSA) or county.\nAir quality stations monitor up to six pollutants, which include ozone, carbon monoxide, nitrogen dioxide, sulfur dioxide, lead, and particulate matter (PM) in sizes 10 and 2.5 micrometers. This study will focus on particulate matter, particularly PM2.5 or fine particulate matter. These small particulates are a mix of solids and aerosols that can have negative impacts on human health and the environment. Most fine particles (PM2.5) are derived from the combustion of fossil fuels or organic materials (i.e. wood), but can also form through chemical reactions in the atmosphere CRB. These pollutants can be inhaled and lead to adverse health effects like reduced lung function, or lung cancer IARC.\nAirData uses AQI for some of its reports, as it is an important index for measuring and reporting daily air quality. Based on a scale from 0 to 500, one can tell how clean or polluted the air is in their area. The higher the AQI value, the more polluted the air is, posing a greater risk to people’s health. An AQI of below 100 is seen as satisfactory, and above 100 is unhealthy, first for sensitive groups of people and then for the general public. Anything greater than 300 is considered hazardous for the entire population."
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-loading-and-cleaning",
    "href": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-loading-and-cleaning",
    "title": "Blog 2: Background, Data Cleaning, Data Equity",
    "section": "Data Loading and Cleaning",
    "text": "Data Loading and Cleaning\nData HERE From the EPA website, we filtered according to the following criteria:\n\nPollutant: PM2.5\nYear: 2020\nLocation: MA - Suffolk County\n\n\nYou would need to start with a subset of the data. How did you choose this?\nWe created a variable list for the columns we wanted to keep, which were:\n\nDate: Used to analyze time trends, such as seasonal changes or long-term trends.\nDaily Mean PM2.5 Concentration: Core indicators, important values for measuring air quality.\nUnits: Ensure data consistency (µg/m³)\nLocal Site Name: Helps identify data sources, especially when comparing different sites.\nMethod code: Used for data verification to ensure consistency of measurement methods.\nCounty: Provide context for regional analysis, comparing air quality in different counties.\nSite Latitude and Site Longitude: Used for geographic analysis and visualization.\n\nFinally, we read the downloaded CSV file into R, selected our desired columns, and wrote an RDS file.\n\nlibrary(tidyverse)\n\nvar &lt;- c(\"Date\",\"Daily Mean PM2.5 Concentration\",\"Units\",\"Local Site Name\", \"Method Code\",\"County\",\"Site Latitude\",\"Site Longitude\")\n\nair_qual &lt;- read_csv(here::here(\"dataset\", \"epa_air_qual.csv\")) |&gt;\n  select(all_of(var))\n\nwrite_rds(air_qual, file = here::here(\"dataset\", \"air_qual_clean.rds\"))\n\n\n\nAre you starting by removing missing values or focusing on columns with less missing data?\nWe started by focusing on columns with less missing data. For example, we prioritized keeping the data and county because it would be important to make comparisons of air quality levels in different counties. In addition, the date is consistent, so we can make conclusions about season trends that cause air quality to change over time. We are using the daily mean concentration as the measure of central tendency which determines the value that we can use to make comparisons about air quality in different local sites. In effect, the average air quality is a good indicator to determine whether we can conclude if the area has poor or high air quality. We included county and local site names for now because we can compare air quality levels within different sites such as Kenmore or Roxbury. The counties are all suffolk, so we may find this data to not be useful in the future. The latitude and longitude data is important to determine specific locations, and there are no missing values which ensures validity.\nColumns we excluded were:\n\nSource: All the source is coming from the AQS(air quality system) and it’s not helpful for our project.\nPOC: POC is the Parameter Occurrence Code which refers to the different detector devices.\nDaily AQI Value: We keep the PM2.5 Concentration, which is more detailed the AQI index, since AQI has lots of waste gas air.\nDaily Obs Count: It is used for data collection, not for air pollution itself.\nPercent Complete: They are all 100% complete, in that case, we don’t need it for our project.\nAQS Parameter Code and AQS Parameter Description: It is used for distinguishing different instruments that measure the same parameter at the same site. Since all the code is 88101, it is not that useful.\nMethod Description: There is already a Method Code, a specific description is not necessary.\nCBSA Code and CBSA Name: These are used for larger regional breakdown analysis, and we will focus on specific sites.\nState FIPS and County FIPS Code: With the “County” column already in place, the FIPS code is not necessary.\nState: All the data are from Massachusetts."
  },
  {
    "objectID": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-equity",
    "href": "posts/2025-03-17-blog-2/blog2_data_clean.html#data-equity",
    "title": "Blog 2: Background, Data Cleaning, Data Equity",
    "section": "Data Equity",
    "text": "Data Equity\nBased on the Principles for Advancing Equitable Data Practice, we must acknowledge the sensitivity of the data we are using. In particular, we will be utilizing census data for demographics about income and race distribution among particular neighborhoods in Boston (Kenmore, Roxbury, Dorchester). Under the ‘Beneficence’ framework in “Conception,” we recognize that our data will be generalizations about the distributions of race and income within these neighborhoods and do not reflect the individual variability within that area.\nAs for the instrumentation and the collection of data in the EPA AirData dataset, it follows the ‘Beneficence’ framework by not including any personally identifiable information (PII) as it only includes information regarding levels of pollution in the air, not anything involving people. It is also clear about the limitations the data holds, as AQI is just a scale, and people need to interpret it to their own health standards to decide what it is healthy for them. Additionally, census data that will be added to the study also avoids PII.\nFinally, recognizing the limitations of the data is important. The generalization of daily mean PM2.5 exposure is limited in its scope. General air quality trends do not preclude individual health risks. Personal risk is based on length of exposure, pre-existing conditions, access to healthcare, etc. Conclusions of this investigation should not be taken as individual health advice, rather a general exploration of spatial variability."
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 8: A pivot at the end\n\n\n\n\n\nExpanding dataset and improving analysis. \n\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 7: Simple Interactive\n\n\n\n\n\nExploration of model type and initial findings. \n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6: Developing a GLM\n\n\n\n\n\nExploration of model type and initial findings. \n\n\n\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 5: Describing and Combining Datasets\n\n\n\n\n\nWe will explain how we plan to combine our data, initial findings, challenges and future steps. \n\n\n\n\n\nApr 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4: Continuing Data Analysis\n\n\n\n\n\nCleaning census data, addressing trends, and preliminary modelling \n\n\n\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3: Backgrond and Data Analysis(summary, barplots, tables)\n\n\n\n\n\nPreliminary Data Analysis, data background, and barplots for continuous variable, and tables for discrete variables. \n\n\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2: Background, Data Cleaning, Data Equity\n\n\n\n\n\nPreliminary data loading and cleaning, subject and data background, data equity evaluation. \n\n\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Descriptions\n\n\n\n\n\nBrief descriptions of potential datasets and directions for research. \n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Team Elegant Scientists",
    "section": "",
    "text": "This comes from the file about.qmd."
  },
  {
    "objectID": "about.html#hrithik-shivnauth",
    "href": "about.html#hrithik-shivnauth",
    "title": "About Team Elegant Scientists",
    "section": "Hrithik Shivnauth",
    "text": "Hrithik Shivnauth\nHi, I am a senior that is majoring in data science. I love learning about new ideas and I am open to working with this group. Github"
  },
  {
    "objectID": "about.html#cat-mahoney",
    "href": "about.html#cat-mahoney",
    "title": "About Team Elegant Scientists",
    "section": "Cat Mahoney",
    "text": "Cat Mahoney\nCat is a PhD student in the Earth & Environment department. She studies the biogeochemistry of coastal waterways, and is passionate about environmental issues and sustainable development. Github"
  },
  {
    "objectID": "about.html#yana-pathak",
    "href": "about.html#yana-pathak",
    "title": "About Team Elegant Scientists",
    "section": "Yana Pathak",
    "text": "Yana Pathak\nI am a junior studying data science and computer science. I am interested in the intersectionality between the technology industry and others - specifically using technology to solve real life problems. Github"
  },
  {
    "objectID": "about.html#jiaxuan-li",
    "href": "about.html#jiaxuan-li",
    "title": "About Team Elegant Scientists",
    "section": "Jiaxuan Li",
    "text": "Jiaxuan Li\nI’m a junior student studying in Econ & Maths major. I like doing calculation related things and crazy about sports. Github"
  },
  {
    "objectID": "about.html#xiaolong-zhou",
    "href": "about.html#xiaolong-zhou",
    "title": "About Team Elegant Scientists",
    "section": "Xiaolong Zhou",
    "text": "Xiaolong Zhou\nI’m a junior student studying in economics. I love math and reasoning. Github\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Exposure to small particulate matter (PM2.5) can lead to respiratory issues like asthma, irregular heartbeat, and heart attacks EPA. Recent research has demonstrated that people of color generally have a higher exposure to air pollution than white residents in the U.S. Environmental Health Persepectives.\nThis section explores trends in EPA air quality metric PM2.5 concentration across monitoring sites along the East Coast U.S. in addition to demographic trends in neighborhoods associated with measured air quality. The ultimate goal was to compare PM2.5 exposure by race throughout 2020."
  },
  {
    "objectID": "analysis.html#air-quality-by-season",
    "href": "analysis.html#air-quality-by-season",
    "title": "Analysis",
    "section": "Air Quality by Season",
    "text": "Air Quality by Season\nDaily air quality parameters like particulate matter concentration are highly variable, especially on a regional scale. The air quality data for this project was collected in 2020, throughout the COVID-19 pandemic. Lockdowns and travel restrictions, particularly throughout Spring 2020, heavily impacted car travel and industry, major contributors of atmospheric particulate matter. Trends in particulate matter throughout 2020 were explored in air_qual_trends.R.\nThe variability of [PM2.5] over time is represented by an average daily value across all sites. A 7-day rolling average has been included to improve illustration of longer-term trends.\n\nOn a monthly scale, reductions in [PM2.5] can be seen in all states during the early months of the pandemic (March-May). According to the CDC Museum Timeline, most travel restrictions fell within this window.\n\nTo determine the effects of seasonality on particulate matter concentration, a linear mixed model (LMM) was performed. The chosen model allows for variation among neighborhoods (a nested variable within state) and across seasons. An ANOVA confirmed that neighborhood interaction significantly improved model fit.\nAn estimated marginal means (from library(emmeans)) was used to explore trends in PM2.5 concentrations across neighborhoods, states, and seasons. Most neighborhoods demonstrated lowest [PM2.5] during the spring season, as expected. Trends in seasonality tended to vary among states, but within states, neighborhoods often demonstrated consistent patterns."
  },
  {
    "objectID": "analysis.html#neighborhoods-by-race",
    "href": "analysis.html#neighborhoods-by-race",
    "title": "Analysis",
    "section": "Neighborhoods by race",
    "text": "Neighborhoods by race\nVisually, demographic differences between states and within states at the neighborhood level are apparent. To confirm statistical significance, a Kruskal-Wallis test was performed in demographic_trends.R. Percent population of all races differed significantly between states. However, within states neighborhoods were not significantly different by percent population by race."
  },
  {
    "objectID": "analysis.html#exploring-air-quality-trends-by-demographics",
    "href": "analysis.html#exploring-air-quality-trends-by-demographics",
    "title": "Analysis",
    "section": "Exploring air quality trends by demographics",
    "text": "Exploring air quality trends by demographics\nInitially, we aimed to pursue a Gamma model to predict [PM2.5] at particular times of year based on neighborhood demographic characteristics. The right-skewed distribution and increasing variance with increased mean of PM2.5 concentration fit the assumptions of a Gamma model. However, there were also instances of true zero PM2.5 days, which violates the assumptions of this model type.\nInstead, a compound Poisson generalized linear model (CPGLM) was appropriate for this investigation. Two approaches were applied to limit multicollinearity in the data. In each case, variation was controlled by month as well.\n\n1. Simpson Diversity Index\nThe Simpson Diversity Index is a representation of diveristy usually applied to biodiversity (number of species). However, here it has been applied to racial diversity and gives a broad overview of racial heterogeneity as well as racial distribution (evenness) by neighborhood Gregorius and Gillet, 2008.\nProportion by race including white, black, asian, and other (combined native, pacific, etc.) were used to calculate a Simpson diversity score from 0 (no diversity) to 1 (maximum diversity).\n\nmodel_diversity &lt;- cpglm(pm2.5_dailymean ~ month + simpson_diversity)\n\nControlling for variation by month, the diversity model output demonstrates that more racially diverse neighborhoods have significantly less exposure to PM2.5.\nThis result appears counterintuitive at first. However, the Simpson Diversity Index does not account for the actual density of any one group. This could suggest that spatial distribution of more segregated neighborhoods is missed in this model. To dive deeper into these relationships, a predictive CPGLM using relative percent by race was developed.\n\n\n2. Relative Percent\nThis model used percent by race as predictor variables. It is important to note that all results are relative to percent white.\n\nmodel_relWhite &lt;- cpglm(pm2.5_dailymean ~ month + pct_black + pct_asian + \n                          pct_native + pct_pacific + pct_other_race +                                    pct_two_or_more, data = dataset)\n\nResults by Race % Population\n\nPercent Black:\n\nEstimate ~ +0.00072 increase in PM2.5 exposure for each 1% increase in Black population. This increase seems small but is significant on large spatial scales. This result suggests areas with higher percent Black populations have greater risk of PM2.5 exposure, consistent with previous research on racial disparities with air quality metrics.\n\nPercent Asian\n\nEstimate ~ +0.00095 shows a similar increase in PM2.5 exposure in areas with higher Asian populations.\n\nPercent Native American and Percent Pacific Islander\n\nInterestingly, both percent Native American and percent Pacific Islander saw strong negative associations with PM2.5, estimates -0.073 and -0.202, respectively. This could suggest that these populations tend to be grouped in more rural areas or areas with lower industrialization.\n\nPercent Other Race\n\nThere was a small but significant positive association with PM2.5 (estimate +0.0028).\n\nPercent Two or More Races\n\nThis relationship was non-significant (p &gt; 0.05).\n\n\nVisualizing relationships\nThe coefficient estimates from both the Simpson Diversity model and the Relative Percent by race model are shown below.\n\nThis comes from the file analysis.qmd."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "data.html#air-quality-data",
    "href": "data.html#air-quality-data",
    "title": "Data",
    "section": "Air Quality Data",
    "text": "Air Quality Data\nFor this project, we combined data from the EPA AirNow database with US Census data obtained using the tidycensus package. We were interested in how air quality metrics – specifically PM2.5 – changed by location. Trends in air quality by neighborhood could give us insight into air pollution exposure by race across the East Coast.\nOriginally, we had chosen three sited in Boston. however, to improve our investigation, we obtained additional sites in Massachusetts, New York, Pennsylvania, New Jersey, Delaware, and Maryland.\nThe locations of each EPA air quality monitoring station is shown below:\n** Add maps here **\nEach state’s data was downloaded for all of 2020 seperately, and had to be joined together in the load_and_clean_air_qual file. The columns of interest included:\n\nDate: Allowws for trends in air quality parameters over time\nState: To distinguish groups\nGEOID: Location associated with each monitor, for combinging datasets later\nDaily Mean PM2.5: Air quality measurement of particulate matter\nAQI: Overall daily Air Quality Index, includes other parameters\nUnits: for consistency, (µg/m³)\nSite Name\nSide ID\nSite Latitude and Longitude: For location needs\n\nNotably in the data, the distribution of Daily Mean PM2.5 is right skewed. This is frequent in air quality data, as most days have generally low concentrations of particulate matter at a wide geographic scale.\n\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\nIt should also be mentioned that any readings of PM2.5 below zero were considered within the error range of the instrumentation, and set to zero for this analysis."
  },
  {
    "objectID": "data.html#census-data",
    "href": "data.html#census-data",
    "title": "Data",
    "section": "Census Data",
    "text": "Census Data\nGetting appropriate census data was more complicated. First, we needed to identify the appropriate census tracts surrounding each monitoring site. This was done in the identify_tracts file.\nAccording to the EPA, most air quality monitoring sites have a “neighborhood scale” radius, representing a spatial range from 0.5 km to 4 km. Using the minimum range value (0.5 km) we attempted to identify a buffer zone around each monitoring site based on unique latitude and longitude.\nThis included using the sf library and a unit transformation to meters.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\n\n# Census API was included here\n\nair_data &lt;- read_rds(file = here::here(\"dataset\", \"air_qual_clean.rds\"))\n\n# identify individual site points\nsite_points &lt;- air_data |&gt;\n  select(site_id, site_name, state, lat, long) |&gt;\n  distinct()\n\n# reproject map points as sf features\nsite_sf &lt;- st_as_sf(site_points, coords = c(\"long\", \"lat\"), crs = 4326)\nsite_proj &lt;- st_transform(site_sf, crs = 3857) # meters\n\n# use census tracts to get 0.5 km intersecting buffer\nsite_buffers &lt;- st_buffer(site_proj, dist = 500)\n\nOnce buffers were established, the intersecting tracts were extracted from the ACS using the tidycensus package and were joined to the dataframe, creating the new variable neighborhood with the naming convention “state_#” based on the monitoring site it intersects.\n\nunique_states &lt;- unique(site_points$state)\n\ntracts &lt;- map_dfr(unique_states, ~ get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",  \n  state = .x,\n  geometry = TRUE,\n  year = 2020\n))\n\ntracts_proj &lt;- st_transform(tracts, crs = 3857)\n\ntracts_joined &lt;- st_join(tracts_proj, site_buffers, join = st_intersects, left = FALSE)\ntracts_named &lt;- tracts_joined |&gt;\n  group_by(state, site_id) |&gt;\n  mutate(neighborhood = paste0(tolower(state), \"_\", row_number())) |&gt;\n  ungroup()\nair_qual_tracts &lt;- tracts_named |&gt;\n  select(site_id, GEOID, NAME, estimate, neighborhood)\n\nwrite_rds(air_qual_tracts, file = here::here(\"dataset\", \"intersect_tracts.rds\"))\n\nThen, using the intersecting tracts identified, census data could be extracted in load_clean_census.R.\nPopulation characteristics and socioeconomic variables of interest were identified as:\n\nTotal Population\nRace Counts (White, Black, Asian, Pacific Islander, Other, Two or More)\nHispanic or Not Hispanic\nMedian Income\nIncome Below Poverty\n\nCount variables were also used to calculate population percentages."
  },
  {
    "objectID": "data.html#combining-datasets",
    "href": "data.html#combining-datasets",
    "title": "Data",
    "section": "Combining datasets",
    "text": "Combining datasets\nFinally, in the clean_data.R file, the census data, air quality data, neighborhood information (tracts df) were all combined. To simplify data structure, geometry was removed from the large data frame.\n\nlibrary(tidyverse)\n\nrace_vars &lt;- c(\n  \"total_pop\", \"white\", \"black\", \"native\", \"asian\", \"pacific\",\n  \"other_race\", \"two_or_more\", \"hispanic\", \"not_hispanic\"\n)\npct_race &lt;- c(\n  \"pct_white\", \"pct_black\", \"pct_native\", \"pct_asian\", \"pct_pacific\",\n  \"pct_other_race\", \"pct_two_or_more\", \"pct_hispanic\", \"pct_not_hispanic\"\n)\n\ntracts &lt;- read_rds(file = here::here(\"dataset\", \"intersect_tracts.rds\"))\n\nair_qual_data &lt;- read_rds(file = here::here(\"dataset\", \"air_qual_clean.rds\")) |&gt;\n  group_by(month)|&gt;\n  summarize(across())\n\ncensus_data &lt;- read_rds(file = here::here(\"dataset\", \"census_data.rds\")) |&gt; \n  rename(GEOID = FIPS)\n\ntracts_joined &lt;- tracts |&gt; \n  left_join(census_data, by = \"GEOID\")\n\ndata_combined &lt;- air_qual_data |&gt;\n  group_by(site_name, month) |&gt;\n  left_join(tracts_joined, by = \"site_id\") |&gt;\n  select(-NAME,-method_code,-method_desc,-units,-geometry)\n\nwrite_rds(data_combined, file = here::here(\"dataset\", \"air_qual_census.rds\"))\nwrite_rds(tracts_joined, file = here::here(\"dataset\", \"census_neighborhood.rds\"))\n\nPopulation distributions for each neighborhood within the states of interest are shown below.\n\nThis comes from the file data.qmd."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2025-02-24-blog-1/datasets.html",
    "href": "posts/2025-02-24-blog-1/datasets.html",
    "title": "Dataset Descriptions",
    "section": "",
    "text": "Particulate matter, especially fine particles (PM2.5) can adversely affect human health. Specifically, these particles can irritate airways, decrease lung function, aggravate asthma, and lead to pulmonary issues like heart attacks EPA.\nDownload HERE! This data set shows daily measures of various air pollutants across the United States. We hope to focus on particulate matter exposure in neighborhoods around Boston, filtering data before downloading as follows:\n\nPollutant: PM2.5\nYear: 2020\nLocation: MA - Suffolk County\n\nThis will provide us with daily data from three sensors in Kenmore Sq., Roxbury, and Dorchester. We would then compare particulate matter (PM) across 2020 census data to explore potential trends in pollutant exposure by race near Boston. We hypothesize that lower income communities that are more industrialized may have greater risk of PM exposure.\nData set information: * 22 columns, although most are not necessary after filtering (e.g. County, State). * 1695 rows. * Data can be downloaded as a .csv file and we should be able to clean it."
  },
  {
    "objectID": "posts/2025-02-24-blog-1/datasets.html#ma-air-quality-epa-dataset",
    "href": "posts/2025-02-24-blog-1/datasets.html#ma-air-quality-epa-dataset",
    "title": "Dataset Descriptions",
    "section": "",
    "text": "Particulate matter, especially fine particles (PM2.5) can adversely affect human health. Specifically, these particles can irritate airways, decrease lung function, aggravate asthma, and lead to pulmonary issues like heart attacks EPA.\nDownload HERE! This data set shows daily measures of various air pollutants across the United States. We hope to focus on particulate matter exposure in neighborhoods around Boston, filtering data before downloading as follows:\n\nPollutant: PM2.5\nYear: 2020\nLocation: MA - Suffolk County\n\nThis will provide us with daily data from three sensors in Kenmore Sq., Roxbury, and Dorchester. We would then compare particulate matter (PM) across 2020 census data to explore potential trends in pollutant exposure by race near Boston. We hypothesize that lower income communities that are more industrialized may have greater risk of PM exposure.\nData set information: * 22 columns, although most are not necessary after filtering (e.g. County, State). * 1695 rows. * Data can be downloaded as a .csv file and we should be able to clean it."
  },
  {
    "objectID": "posts/2025-02-24-blog-1/datasets.html#nutrition-physical-activity-and-obesity---behavioral-risk-factor-surveillance-system",
    "href": "posts/2025-02-24-blog-1/datasets.html#nutrition-physical-activity-and-obesity---behavioral-risk-factor-surveillance-system",
    "title": "Dataset Descriptions",
    "section": "Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System",
    "text": "Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System\nDownload HERE! This dataset includes data on adult’s diet, physical activity, and weight status from Behavioral Risk Factor Surveillance System. We can focus on nutrition and obesity in MA. We can filter the data by:\n\nYear: 2022\nData_value_type: Value\nLocationAbbr: MA\n\nWe will see several lines of data related to topics such as eating habits and weight indicators (obesity, overweight), which reflect the value of a certain nutrition or weight indicator in Massachusetts in 2022, and the corresponding confidence intervals, stratified categories, and so on. Further aggregating or visualizing these data will help us understand the nutrition and obesity status of Massachusetts adults in 2022."
  },
  {
    "objectID": "posts/2025-02-24-blog-1/datasets.html#u.s.-deaths-by-suicide-cdc-dataset",
    "href": "posts/2025-02-24-blog-1/datasets.html#u.s.-deaths-by-suicide-cdc-dataset",
    "title": "Dataset Descriptions",
    "section": "U.S. Deaths by Suicide: CDC Dataset",
    "text": "U.S. Deaths by Suicide: CDC Dataset\nAccording to the CDC, suicide is among the leading causes of death in the US, with rates steadily rising since 2000. Data also shows distinct demographic trends in suicide rates based on both race and gender.\nThe following data set shows death rates by suicide by sex, race, Hispanic origin, and age from 1950 to 2022. We hope to identify longer term demographic trends in these data, and perhaps find time series economic or other data sets to identify potential drivers in changing rates of death by suicide over the years.\nDownload HERE\nData set information: * These data are collected from annual public use mortality files from the National Center for Health Statistics and the U.S. Census Bureau. * 6,390 rows. * 13 columns. * The issue with this data set is the structure of the columns. They combine multiple variables (e.g. Age, Sex, and Hispanic origin) into one cell when they should be separate columns."
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html",
    "href": "posts/2025-03-24-blog-3/blog-3.html",
    "title": "Blog 3: Backgrond and Data Analysis(summary, barplots, tables)",
    "section": "",
    "text": "The EPA AirData project tracks air quality in the US, collecting data on pollutants that are linked to serious health risks. Specifically PM2.5 particles. These small particles, primarily from fossil fuel combustion and atmospheric chemical reactions, are known to impair lung function and increase cancer risks. The 2024 study in Nature Communications (https://www.nature.com/articles/s41467-023-43492-9), showed in class, analyzed US air pollution trends from 1970 to 2010, showing that there were reductions in pollution due to efforts like the Clean Air Act. It also showed disparities, specifically how pollutant exposure varies by socioeconomic factors, as some communities experience slower improvements. Our study examines Boston’s PM2.5 levels using data from Roxbury, Kenmore Square, and Dorchester to focus on local trends. We are extending the timeline to the present, as we aim to identify emerging disparities and evaluate the continued effectiveness of air quality regulations."
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#data-background",
    "href": "posts/2025-03-24-blog-3/blog-3.html#data-background",
    "title": "Blog 3: Backgrond and Data Analysis(summary, barplots, tables)",
    "section": "",
    "text": "The EPA AirData project tracks air quality in the US, collecting data on pollutants that are linked to serious health risks. Specifically PM2.5 particles. These small particles, primarily from fossil fuel combustion and atmospheric chemical reactions, are known to impair lung function and increase cancer risks. The 2024 study in Nature Communications (https://www.nature.com/articles/s41467-023-43492-9), showed in class, analyzed US air pollution trends from 1970 to 2010, showing that there were reductions in pollution due to efforts like the Clean Air Act. It also showed disparities, specifically how pollutant exposure varies by socioeconomic factors, as some communities experience slower improvements. Our study examines Boston’s PM2.5 levels using data from Roxbury, Kenmore Square, and Dorchester to focus on local trends. We are extending the timeline to the present, as we aim to identify emerging disparities and evaluate the continued effectiveness of air quality regulations."
  },
  {
    "objectID": "posts/2025-03-24-blog-3/blog-3.html#data-analysis",
    "href": "posts/2025-03-24-blog-3/blog-3.html#data-analysis",
    "title": "Blog 3: Backgrond and Data Analysis(summary, barplots, tables)",
    "section": "Data Analysis",
    "text": "Data Analysis\nDaily Mean PM2.5 Concentration The Daily Mean PM2.5 Concentration is a key indicator of air quality. Based on our data, the average concentration is 6.801712 μg/m³, with a median of 6.3 μg/m³. The values range from 42.33 to 42.35, and the standard deviation is 3.264045. The interquartile range (IQR) is 3.9, showing the spread of the middle 50% of the data.\nThe Site Latitude The Site Latitude values describe the north-south positioning of the air quality monitoring sites. The latitudes range from -71.10 to -71.06, with a mean of -42.33273 and median of 42.3295. This indicates that the stations are located across a wide vertical geographical spread.The standard deviation is 0.02368671, and the IQR is 0.02368671, suggesting varying coverage in the north-south direction.\nThe Site Longitude The Site Longitude captures the east-west geographic spread of monitoring sites. The mean longitude is -71.07442, median is -71.0826, and the values range from -71.09716 to -71.05606. The standard deviation is 0.01762767.\n\n# 1. Scatter plot of PM2.5 vs AQI value\nggplot(epa_data, aes(x = `Daily Mean PM2.5 Concentration`, y = `Daily AQI Value`)) +\n  geom_point(alpha = 0.5, color = \"darkblue\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Relationship Between PM2.5 Concentration and Air Quality Index (AQI)\",\n       x = \"PM2.5 Concentration\",\n       y = \"Air Quality Index\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nScatter plot of PM2.5 vs AQI value This scatter plot shows the relationship between the daily average concentration of PM2.5 (X-axis) and the corresponding Air Quality Index (AQI) value (Y-axis). Each blue dot represents a day’s measurement. The red line is a linear regression line showing the overall trend. The strong positive correlation visible in this graph shows that as PM2.5 concentrations increase, AQI values also increase in a predictable manner. This relationship is to be expected, as PM2.5 is the main component in calculating AQI values.\n\n\n# 2. Bar chart of PM2.5 by month (seasonal patterns)\nepa_data &lt;- readRDS(\"dataset/air_qual_clean.rds\")\nhead(epa_data$Date)\n\n[1] \"1/1/2020\"  \"1/4/2020\"  \"1/7/2020\"  \"1/10/2020\" \"1/13/2020\" \"1/16/2020\"\n\nepa_data$Date &lt;- as.Date(epa_data$Date)\n \nepa_data$Month &lt;- as.numeric(format(epa_data$Date, \"%m\"))\n \nmonthly_avg &lt;- epa_data |&gt;\n  group_by(Month) |&gt;\n  summarize(avg_pm25 = mean(`Daily Mean PM2.5 Concentration`, na.rm = TRUE))\n \nggplot(monthly_avg, aes(x = factor(Month), y = avg_pm25)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Monthly Variation in PM2.5 Concentration\",\n       x = \"Month\",\n       y = \"Average PM2.5 Concentration\") +\n  scale_x_discrete(labels = month.abb) \n\n\n\n\n\n\n\n\n\nBar chart of PM2.5 by month (seasonal patterns) This bar chart shows the average PM2.5 concentration for each month of the year. The X-axis shows the month and the Y-axis shows the average concentration of PM2.5. This visualization reveals seasonal patterns of air pollution. You can see which months typically have higher or lower PM2.5 levels. Typically, the winter months show higher concentrations due to increased heating and temperature inversions of trapped pollutants, while the summer months may have lower concentrations.\n\n\n# 3. Bar chart of observation count by monitoring method\nmethod_counts &lt;- epa_data |&gt;\n  group_by(`Method Description`) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\nggplot(head(method_counts, 10), aes(x = reorder(`Method Description`, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  coord_flip() +\n  labs(title = \"Frequency of Monitoring Methods (Top 10)\",\n       x = \"Monitoring Method\",\n       y = \"Number of Observations\") \n\n\n\n\n\n\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 3. Bar chart of observation count by monitoring method This bar chart shows the ten most commonly used methods for monitoring PM2.5 concentrations. The Y-axis lists the different monitoring method descriptions, and the X-axis shows the count of observations made using each method. This visualization is important for understanding data collection methods. Different monitoring methods may have different levels of accuracy, detection limits, or biases. Datasets dominated by one or both methods provide more consistent measurements, while more diverse method sets may introduce variability that needs to be taken into account when interpreting results.\n\ndiscrete_vars &lt;- sapply(epa_data, function(x) is.factor(x) || is.character(x) || is.logical(x))\ndiscrete_data &lt;- epa_data[, discrete_vars]\n\nggplot(epa_data, aes(x = `Local Site Name`)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Distribution of Air Quality Event Types\",\n       x = \"Local Site Name\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nThis is a picture that shows the different number of locations throughout 2020. As we can see RoxBury and Boston Kenmore SQ are even, which is about 450, and Von Hillern Street has about 800 different locations for Air quality values."
  },
  {
    "objectID": "posts/2025-03-31-blog-5/blog-5.html",
    "href": "posts/2025-03-31-blog-5/blog-5.html",
    "title": "Blog 5: Describing and Combining Datasets",
    "section": "",
    "text": "Datasets combined:\nWe brought together two primary datasets:\nEPA Daily PM2.5 Air Quality Data (2020): This includes daily mean concentrations of fine particulate matter collected at monitor stations across Boston. It looks at the AQI value to determine how polluted the air is in an area. These values can range from 0-500 where values above 100 are unhealthy and values below 100 or below are satisfactory. Any value greater than 300 is hazardous for the entire population. Daily means are the basis for epidemiological time‑series studies, health warnings, and regulatory compliance. For example, this information can be used to quantify neighborhood exposure disparities, evaluate the impact of policy interventions such as lockdowns and validate low cost sensor networks to have better air quality.\nU.S. Census Data: We pulled demographic and housing data from the Decennial Census DHC release and socioeconomic characteristics from the 5-Year American Community Survey (2015–2020). Since most sensors operate at a neighborhood scale of 0.5-4 km, we identified census tracts which would intersect a 0.5 km near each sensor location. The purpose of this was to provide a consistent geography for tabulating and comparing demographic, social, economic and housing data from one decennial census (and the American Community Survey) to the next.\n\n\nHow we’re combining:\nSince our air quality data is available at a daily time scale and census data is not as frequent, we have combined the datasets spatially rather than time-wise. We aligned air quality monitor locations with nearby census tracts, creating neighborhood profiles for each monitoring site. This allows us to do comparative analysis across areas like Roxbury, Dorchester, and Kenmore. We also can link PM 2.5 monitors to census tracts in Boston. We can locate each monitor by using the longitude and latitude from the EPA file. Then we can overlay on a tract shape file to find which tract contains that point. Afterwards we can join ACS data such as the % of residents below the poverty line or racial composition to the tract. We combined the air_qual and census data using a left_join where we joined by the site name and used a many to many relationship.\n\n\nInitial findings:\nWith the joined dataset, we have begun early data analysis. A few early observations include:\n\nKenmore Square has a significantly higher percentage of white residents compared to Dorchester and Roxbury.\nKenmore showed higher annual PM2.5 concentrations overall, while Roxbury had slightly lower levels on average.\nSpring 2020 recorded the lowest PM2.5 levels across all neighborhoods—maybe influenced by COVID-19 lockdowns and reduced activity during that time.\n\n\n\nChallenges:\nThe time scale of our datasets are very different. The census data and the epa data are within 2020, but the ACS data represents a 5-yr average from 2015-2020. Though all of them are within 2020 so it is possible to join, finding census tracts which are deterministic may be inconclusive if we do not select a year that is significant.\n\n\nNext steps:\nWe’ve successfully downloaded, cleaned, and joined our datasets. We’ve grouped air quality data by station-neighborhood and are in the process of modeling PM2.5 exposure using a linear model to compare concentrations across sites and time. We also want to analyze exposure disparities by comparing daily PM 2.5 levels or annual averages across tracts with different socioeconomic profiles. Our next step is to build a predictive gamma model to examine how demographic and socioeconomic factors may be associated with exposure to higher levels of particulate matter. This will help us better understand potential environmental inequities in pollution exposure across Boston."
  },
  {
    "objectID": "posts/2025-04-22-blog-7/blog-7.html",
    "href": "posts/2025-04-22-blog-7/blog-7.html",
    "title": "Blog 7: Simple Interactive",
    "section": "",
    "text": "Introduction\nFor this week’s post, I want to dive into the interactive component of our final project. The aim is to create an engaging, user-friendly visualization that allows users to explore both the big picture trends and zoom in on specific subsets of data. Interactivity will be key, as it encourages users to interact directly with the data and discover meaningful insights on their own. This post will outline the thoughts and progress made so far with the interactive and how it connects to our original hypothesis.\n\n\nUser Experience and Interactions\nOur interactive visualization should provide users with multiple levels of exploration. At the highest level, users will see broad trends or aggregated statistics, such as overall averages or comparisons. However, we also want to allow users to drill down into more specific data points. This functionality allows users to zoom in on individual observations and subsets, such as different geographic locations, demographic groups, or time periods, depending on the data context.\nTo achieve this, I plan to implement interactive components that allow for filtering and selection. Users will be able to select from various variables to focus their exploration. Additionally, the interactive should provide clear guidance on how to use these features. This could include tooltips, instructions, or highlighted findings that guide users toward interesting patterns or anomalies.\nThe user experience should feel personal when possible. For example, if the data is relevant to a specific geographic location or demographic, we could include features where users can input their own data (e.g., entering a zip code) to view trends that are personally relevant.\n\n\nNew Interactive Idea: Air Quality Lookup Based on Birthday\nIn lecture on Friday, we brainstormed a new interactive idea that could be both fun and informative. The idea involves an air quality lookup tool for Boston, specifically focused on PM2.5 levels. This tool would allow users to input their birthday (or any date of their choosing) and see what the air quality was like in Fenway Park (or any location of interest) on that day.\nFor example, if my birthday is June 20, 2020, I could input that date and look up the air quality in Fenway on that day to see what the PM levels were like, helping me connect personal memories to environmental data. This type of interactive tool could be engaging because it allows people to explore air quality data in a way that feels personally relevant to them. It brings the data closer to the user by tying it to significant dates or locations in their lives.\nThe interactive component could display the PM2.5 levels overlaid on a map or graph for the selected location and date. Additionally, users could see trends over time (e.g., comparing air quality across different birthdays or other important dates in their lives).\n\n\nSteps to Implement the Idea\n\nSetting Up the Framework: The first step will be to set up the interactive framework using R. I plan to use shiny, an R package that allows for the creation of interactive web applications. We will define input controls like dropdowns, sliders, or text boxes that users can manipulate to filter the data.\nConnecting Inputs and Outputs: For each input the user provides, we will dynamically generate output that reflects their selections. This might include:\n\nUpdating charts based on the selected date and location.\nDisplaying a map or a time series showing air quality data for the chosen birthday.\n\nThese updates will be achieved using reactive expressions in shiny. A reactive expression automatically updates the output when the user changes the input.\nExploration and Display: We will need to determine which visuals and types of data to display at various levels of user interaction. This might include:\n\nHigh-level visualizations (e.g., average air quality over time).\nDrill-down views (e.g., specific day’s PM2.5 levels at Fenway Park).\n\nPersonalization and User Engagement: If possible, we want to give the user a sense of personal engagement. For example, users could enter their birthday or any date, and the visualizations could reflect air quality trends for that day. This creates a more personalized experience that connects the data to the user’s own life.\n\n\n\nConnection to Original Hypothesis\nThe interactive visualization will help test and explore the hypotheses we developed earlier in the project. Specifically, it will allow users to explore how environmental factors like air quality vary on specific days, potentially influencing their health or experiences. For example, if the hypothesis is about how air quality affects people’s daily lives, allowing users to explore the data on their own birthday could help bring the data to life and make it more tangible.\n\n\nInputs and Outputs\nThe inputs for the interactive will include: - Text input for users to enter their birthday or any date of interest. - Dropdown menu for selecting the location (e.g., Fenway Park, other areas in Boston). - Date picker for users to select a custom date.\nThe outputs will include: - Dynamic visualizations showing air quality data, such as a graph of PM2.5 levels for the selected date and location. - Summary statistics such as average PM2.5 levels for the chosen date range or location. - Map visualization showing the location of monitoring stations and PM2.5 concentrations.\nThe interaction works in such a way that when a user inputs their birthday (or any date), the corresponding output will be recalculated and displayed. This makes the process seamless and intuitive, allowing users to directly connect with the data.\n\n\nCode Implementation\nHere’s a basic structure of how the code for the interactive might look using shiny:\n```r library(shiny) library(ggplot2) library(lubridate)\n\n\nSample air quality data\ndata &lt;- data.frame( date = seq(ymd(“2020-01-01”), ymd(“2020-12-31”), by = “days”), pm25 = rnorm(366, mean = 10, sd = 5), # Random PM2.5 levels location = rep(“Fenway Park”, 366) )\n\n\nUI definition\nui &lt;- fluidPage( titlePanel(“Air Quality Lookup”), sidebarLayout( sidebarPanel( dateInput(“birthday”, “Enter Your Birthday:”, value = “2020-06-20”), selectInput(“location”, “Choose Location:”, choices = unique(data$location)) ), mainPanel( plotOutput(“air_quality_plot”), textOutput(“air_quality_summary”) ) ) )\n\n\nServer logic\nserver &lt;- function(input, output) {\nselected_data &lt;- reactive({ data[data\\(date == input\\)birthday & data\\(location == input\\)location, ] })\noutput$air_quality_plot &lt;- renderPlot({ ggplot(selected_data(), aes(x = date, y = pm25)) + geom_line() + labs(title = “PM2.5 Levels on Your Birthday”, x = “Date”, y = “PM2.5 (µg/m³)”) + theme_minimal() })\noutput\\(air_quality_summary &lt;- renderText({\n    paste(\"PM2.5 Level on\", input\\)birthday, “:”, selected_data()$pm25) }) }\n\n\nRun the application\nshinyApp(ui = ui, server = server)\n\nConclusion\nThis interactive component will significantly enhance user engagement by allowing exploration of the data through personal inputs and dynamic visualizations. The idea of looking up air quality data on a user’s birthday (or any significant date) makes the experience feel more personal and relevant. As we move forward, we will refine the interactions and ensure that the design aligns with the original goals of the project. The ability to explore data and discover insights interactively is crucial to the success of the final project, and I look forward to seeing how the interactive evolves."
  }
]